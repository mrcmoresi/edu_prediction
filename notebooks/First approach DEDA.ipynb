{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analyzing the possible submission status in introalg-2c-2017 database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we analyse the possible values that the submssion status can take. We also calculate how frequent they are in the introalg-2c-2017 dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up\n",
    "First of all we load the libraries that we need for the code below. **Json** is the format in which the database is stored. **Pandas** is a python library for handling databases in the memory of the computer. Pandas store the data in structures called **dataframes**. **Os** is a library used to gain portability to other operating systems, in this way the notebook will work in Ubuntu, Windows, etc.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load database\n",
    "\n",
    "LB: Avoid using comments in the text as you do below. Comments tend to be short and not human friendly :). Put a Markdown cell as this one before each cell of code and explain what the code below it does. Replace what I wrote here by an explanation of what is going on below. Explain what it means to expand and print some sample submission so that we see the shape of the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load Dataset  \n",
    "dataset_home = 'datasets'\n",
    "json_file = open(os.path.join(dataset_home, '2017-2c-introalgo-2c-2017.json'), encoding='utf8')\n",
    "json_data = json.load(json_file)\n",
    "\n",
    "#train_df = pd.io.json.json_normalize(json_data) #LB: this variable is never used, do not define variables that you do not use\n",
    "\n",
    "#Expand and show the first submissions in the Dataframe \n",
    "submissions = pd.io.json.json_normalize(json_data, record_path='submissions')\n",
    "submissions.columns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = set([])\n",
    "#calculate possible status value for submissions\n",
    "for elem in submissions.status:\n",
    "    results.add(elem)\n",
    "print(\"Submissions status {}\".format(results))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#count amount of submmissions\n",
    "total_amount_submissions = submissions.shape[0]\n",
    "total_amount_submissions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Obtain frequency of submission grouped by status\n",
    "submissions_grouped = submissions.groupby(['status']).size()\n",
    "metrics = {}\n",
    "metrics = submissions_grouped / total_amount_submissions\n",
    "metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics in courses\n",
    "Calculate status distribution in all submissions in assignments "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_home = 'datasets/courses/'\n",
    "\n",
    "df = pd.DataFrame(\n",
    "    columns=['course', 'submission_amount', 'errored', 'failed', 'passed', 'passed_with_warnings'])\n",
    "\n",
    "metrics_list = []\n",
    "for course in os.listdir('datasets/courses/'):\n",
    "    json_file = open(os.path.join(dataset_home, course), encoding='utf8')\n",
    "    json_data = json.load(json_file)\n",
    "    submissions = pd.io.json.json_normalize(json_data, record_path='submissions')\n",
    "    submissions = submissions[~(submissions['status'] == 'aborted')]\n",
    "    submissions = submissions[~(submissions['status'] == 'pending')]\n",
    "    submissions = submissions[~(submissions['status'] == 'manual_evaluation_pending')]\n",
    "    total_amount_submissions = submissions.shape[0]\n",
    "    submissions_grouped = submissions.groupby(['status']).size()\n",
    "    metrics = {}\n",
    "    metrics = submissions_grouped / total_amount_submissions\n",
    "    metrics['course'] = str(course)\n",
    "    metrics['submission_amount'] = total_amount_submissions\n",
    "    df = df.append(metrics, ignore_index=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = (df[[\"errored\", \"failed\", \"passed\", \"passed_with_warnings\"]].multiply(df[\"submission_amount\"], axis=\"index\")).sum()/(df['submission_amount']).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean['course'] = 'Promedios'\n",
    "mean['submission_amount'] = df['submission_amount'].sum()\n",
    "df.loc[len(df)] = mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
