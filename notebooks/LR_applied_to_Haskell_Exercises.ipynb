{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "udcePd9k9259"
   },
   "source": [
    "# Linear Regression applied to Haskell Exercises from \"Failed_Submissions\" Collection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_WbujCwg96ii"
   },
   "source": [
    "In this notebook we present the results of our first approach on training a machine learning model, based on set of submissions from Haskell exercises on the Mumuki Collection ** failed_submissions** in order to automatically classify programming exercises into non executable (dark red), executable with errors (light red), correct but quality could be improved (yellow), good solution (green) . \n",
    "\n",
    "In section 1, we describe Haskell Exercise's Submissions Datasets and some filters we had make on the submissions. In section 2 we present the model we trained and the results we obtained from different excersises. Finally, in section 3 we present differents Tokenizer options for the Vectorizer, and show how they tokenize the submissions content.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bRnrxbfG9JZt"
   },
   "source": [
    "##  Techinical Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G-584PVY-Rxt"
   },
   "source": [
    "On the cells of this unnumbered section we define the code that it's necessary to execute to satisfy all the requirments for training the models for the differents exercises.\n",
    "\n",
    "On the code below, we install the spacy library, becuase we test different kind of Tokenizers for the Vectorizer.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "DA0-xw0O-ODh"
   },
   "outputs": [],
   "source": [
    "!pip install spacy\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "9k8FbIBD9JZx"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Q9ObgCvz9JZ_"
   },
   "source": [
    "## 1 The Failed Submissions Collection Dataset\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "b7V2u9WI-zjx"
   },
   "source": [
    "###  1.1The Failed Submissions Collection\n",
    "\n",
    "We decided to work with submissions that are part of the collection *failed_submissions.* Failed Sumbissions is an specific submissions collection from mumuki. The submissions that form part of this collection were made by people that are not enrolled at formal education courses. So people use directly the mumuki.io guides to learn by them self. The mumuki.io platform has guides with different programming languages. As failed_submissions collection has many submissions, we decide first to filter submission by programming language. We decide to use Haskell as programming language. Also, we decide to divide the Haskell submissions by Exercise. So we are going to have an specific dataset per exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "cntISiMjAPSm"
   },
   "source": [
    "### 1.2 Analize the Dataset\n",
    "\n",
    "Every submission has a field called ** status **. Depend of the code that student had submitted, the value of this field. \n",
    "\n",
    "1.  **errored** : the code not compile. \n",
    "2.  **failed**: the code not pass all the tests defined by the teacher.\n",
    "3.  **passed_with_warnings**: the code pass all the tests, but on the solution the studennt is not using an specific concept that teacher ask for (expectatives)\n",
    "4.  ** passed**: the code pass all tests and uses the concepts the teacher ask for.\n",
    "4. ** aborted ** : server errors or code that last to much to execute.\n",
    "5. ** pending **: we have the figure up what really do\n",
    "6. ** manual_evaluation_pending **:  we have the figure up what really do\n",
    "\n",
    "In the code below we load the submissions per Haskell exercise to do a quantitative analysis in order to obtain status distribution by exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "qgLjnYjjSVFt"
   },
   "outputs": [],
   "source": [
    "dataset_home = 'datasets/haskell/'\n",
    "\n",
    "submissions_df = pd.DataFrame()\n",
    "\n",
    "for course in os.listdir('datasets/haskell/'):\n",
    "    json_file = open(os.path.join(dataset_home, course), encoding='utf8')\n",
    "    json_data = json.load(json_file)\n",
    "    submissions = pd.io.json.json_normalize(json_data)\n",
    "    submissions_df = submissions_df.append(submissions)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "XWi6gHaSSou8"
   },
   "source": [
    "We decide to analyze the distribution of the exercises per status.  On the next table we can see the distribution of submission status by exercise. We use the *extract_metrics* function to calculate the distribution of the submissions by status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "xMYfQIEhAueh"
   },
   "outputs": [],
   "source": [
    "def calculate_distribution(df, column_name, exercise):\n",
    "    \"\"\"\n",
    "    df: Dataframe with submissions\n",
    "    column_name: column with status\n",
    "\n",
    "    Function to obtain distribution of submissions status\n",
    "    return metrics and amount of submissions\n",
    "    \"\"\"\n",
    "    total_amount_submissions = df.shape[0]\n",
    "    submissions_grouped = df.groupby([column_name]).size()\n",
    "    metrics = {}\n",
    "    metrics = submissions_grouped\n",
    "    metrics['exercise'] = str(exercise)\n",
    "    metrics['submission_amount'] = total_amount_submissions\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def weighted_mean(df, columns_to_mean, amount):\n",
    "    \"\"\"\n",
    "    df: Dataframe with submissions\n",
    "    columns_to_mean: columns to consider to be divided\n",
    "    amount: pounded\n",
    "\n",
    "    return add the weighted mean of status submission\n",
    "    in the dataframe\n",
    "    \"\"\"\n",
    "    df_mean = (df[columns_to_mean].astype(float).multiply(df[amount], axis=\"index\")).sum()/(df[amount]).sum()\n",
    "    df_mean['exercise'] = 'Weighted Mean'\n",
    "    df_mean['submission_amount'] = df['submission_amount'].sum()\n",
    "    df.loc[len(df)+1] = df_mean\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "wJpfOC7AUSZa"
   },
   "outputs": [],
   "source": [
    "distribution_by_exercise = pd.DataFrame(\n",
    "    columns=[\n",
    "        'exercise', 'submission_amount', 'aborted','errored', 'failed',\n",
    "        'passed', 'passed_with_warnings', 'pending', 'running'])\n",
    "\n",
    "exercises = submissions_df['exercise.name'].unique()\n",
    "\n",
    "for exercise in exercises:\n",
    "    df_exercise = submissions_df.where(submissions_df['exercise.name']==exercise).dropna(axis=0, how='all')\n",
    "    distribution_by_exercise = (\n",
    "        distribution_by_exercise.append(\n",
    "            calculate_distribution(df_exercise, 'status', exercise), ignore_index=True).fillna(value=0))\n",
    "\n",
    "#save a copy\n",
    "status_by_exercises = distribution_by_exercise.drop(['aborted', 'pending','running'], axis=1, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_by_exercise.loc[:, 'aborted':] = distribution_by_exercise.loc[:, 'aborted':].div(distribution_by_exercise.iloc[:]['submission_amount'], axis=0)\n",
    "distribution_by_exercise = weighted_mean(distribution_by_exercise, distribution_by_exercise.columns[2:], 'submission_amount')\n",
    "distribution_by_exercise.sort_values('submission_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HCnrB7lCWR46"
   },
   "source": [
    "### 1.3 Cleaning the Datasets\n",
    "\n",
    "We're interested spcifiically in four status values: **errored **, ** failed**, ** passed_with_warnings** and ** passed **. We droped all the submissions with 'manual_pending' 'pending', 'aborted', 'running' status, because them don't provide significative data about the code and the feedback is related with server or technical problems of the Mumuki platform and they were'nt statiscally significant. We use the function * clean_submissions * to clean the dataset. \n",
    "\n",
    "On the table below we could see the results of the new status distribution of the exercises."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "1WV8NKD8b9Or"
   },
   "outputs": [],
   "source": [
    "def clean_submissions(submissions_df, to_train=False):\n",
    "    \"\"\"\n",
    "    submissions_df: Dataframe with submissions\n",
    "    to_train: indicate if dataframe will be use for training\n",
    "\n",
    "    Function to cleaning dataset\n",
    "    \"\"\"\n",
    "    submissions_df = submissions_df[~(submissions_df['status'] == 'aborted')]\n",
    "    submissions_df = submissions_df[~(submissions_df['status'] == 'pending')]\n",
    "    submissions_df = submissions_df[~(submissions_df['status'] == 'running')]\n",
    "    submissions_df = submissions_df[~(submissions_df['status'] == 'manual_evaluation_pending')]\n",
    "    \n",
    "    if to_train:\n",
    "            submissions_df = submissions_df[submissions_df['content'] != \"\"]\n",
    "            submissions_df = submissions_df[~submissions_df['content'].isnull()]\n",
    "    return submissions_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "gFBnZYPscbmr"
   },
   "outputs": [],
   "source": [
    "status_by_exercises['submission_amount'] = status_by_exercises[['errored', 'failed', 'passed', 'passed_with_warnings']].sum(axis=1)\n",
    "status_by_exercises.loc[:, 'errored':'passed_with_warnings'] = status_by_exercises.loc[:, 'errored':'passed_with_warnings'].div(status_by_exercises.iloc[:]['submission_amount'], axis=0)\n",
    "status_by_exercises = weighted_mean(status_by_exercises, status_by_exercises.columns[2:], 'submission_amount')\n",
    "status_by_exercises.sort_values('submission_amount')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "F-QSU1t-gGzC"
   },
   "source": [
    "As we're going to use as model for training Linear Regression with a vectorizer, we have to consider some specific cases. We droped submissions with empty content because them would cause issues with vectorizer. If we consider submissions with empty conent, we have two different cases:\n",
    "\n",
    "1. Mumuki offers some informative exercises where students  don't need to code and the content field is null. In this case when we filter the null content submissions, we will eliminate all the submissions from this particular exercise.\n",
    "2.  In any exercises student could submit an empty solution. It's a border case that we prefer not to consider in this first approach.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#limpio submissions\n",
    "submissions_df_cleaned = clean_submissions(submissions_df, to_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TtuxLHgecl0-"
   },
   "source": [
    "### 1.4 Train, Dev and Test Set\n",
    "\n",
    "First of all we take a technical decistion. We decide not to train all the exercises that have less than 100 submissions. We think that we're not statiscally signifivatives.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exercises_to_train = status_by_exercises[status_by_exercises['submission_amount'] > 100].sort_values('submission_amount')[:-1]\n",
    "exercises_to_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "In this particular case we decided to split the exercises dataset only in train and test. We don't use dev set because the number of submissions per exercises is small.\n",
    "\n",
    "In the code below, we can see the generation of train and test datasets for one exercise.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "rT085oKRs1DN"
   },
   "outputs": [],
   "source": [
    "exercise_df = submissions_df_cleaned.where(submissions_df_cleaned['exercise.name']=='intersectar').dropna(axis=0, how='all')\n",
    "X = exercise_df['content']\n",
    "Y = exercise_df['status']\n",
    "\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.40, random_state=32)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2EXQXjfdtGND"
   },
   "source": [
    "\n",
    "We try to maintain the same status distribution in train and test set as a good practice to train a machine learning model. Test and Train sets have at least one example of each status to make sure when we will train/test our model with train set it contains examples of all types of status.\n",
    "\n",
    "In the code below we show a particular exercise distribution of train and test as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "8L9dy-XeuMJw"
   },
   "outputs": [],
   "source": [
    "print(\"Train Distribution\")\n",
    "print(Y_train.value_counts()/Y_train.shape[0])\n",
    "\n",
    "print(\"\\n Test Distribution\")\n",
    "print(Y_test.value_counts()/Y_test.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vPfrbMyru3p9"
   },
   "source": [
    "## 2 Choose Model and Training\n",
    "\n",
    "We choosed Linear Regression as first approach model because is the easiest model to train. We select it as our baseline, and will try to obtain the best performance classifing submissions setting parameters and selecting tokenizer. In case that not obtain a good performance we are going to change ML model. \n",
    "\n",
    "As we describe in ** Cleaning the Dataset** section, we use *CountVectorizer* for convert submissions content into features and *LabelEncoder* to convert an status into a number. ** Status**. On this first approach the **status** will be target for our machine learning model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "v2ZRt4GZxlpl"
   },
   "source": [
    "### 2.1 One vs Rest Algorithm\n",
    "If we train a multiclass classifier, we need to use [one-vs-rest algorithm](https://en.wikipedia.org/wiki/Multiclass_classification#One-vs.-rest). In a few words, create a binary classifier (Linear Regresssion) for each status type, classify the example by all the classifiers and get the max of those predictions as predicted label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Featurize submissions content\n",
    "cv = CountVectorizer(lowercase=False)\n",
    "X_transformed = cv.fit_transform(X_train)\n",
    "\n",
    "#Featurize status submissions\n",
    "le = LabelEncoder()\n",
    "Y_transformed = le.fit_transform(Y_train)\n",
    "\n",
    "#Train linear regression for multi label classification\n",
    "lr = OneVsRestClassifier(LinearRegression())\n",
    "lr.fit(X_transformed, Y_transformed)\n",
    "\n",
    "#classify Test set\n",
    "predicted = lr.predict(cv.transform(X_test))\n",
    "\n",
    "#obtain metrics avg/total\n",
    "#shape of metrics\n",
    "#precision    recall  f1-score   amount_tested\n",
    "metrics_by_exercise = classification_report(le.transform(Y_test), predicted, target_names=le.classes_, digits=2)\n",
    "\n",
    "print(metrics_by_exercise)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One vs Rest applied to all exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_performance = pd.DataFrame(columns=['exercise', 'precision', 'recall', 'f1-score', 'amount_tested', 'submission_amount'])\n",
    "i = 0\n",
    "exercises = exercises_to_train['exercise'].unique()\n",
    "not_trained = []\n",
    "for exercise in exercises:\n",
    "    #Split dataset in train and test\n",
    "    try:\n",
    "        exercise_df = submissions_df_cleaned.where(submissions_df_cleaned['exercise.name']==exercise).dropna(axis=0, how='all')\n",
    "        X = exercise_df['content']\n",
    "        Y = exercise_df['status']\n",
    "\n",
    "        X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=0.40, random_state=32)\n",
    "\n",
    "        #Featurize submissions content\n",
    "        cv = CountVectorizer(lowercase=False)\n",
    "        X_transformed = cv.fit_transform(X_train)\n",
    "\n",
    "        #Featurize status submissions\n",
    "        le = LabelEncoder()\n",
    "        Y_transformed = le.fit_transform(Y_train)\n",
    "\n",
    "        #Train linear regression for multi label classification\n",
    "        lr = OneVsRestClassifier(LinearRegression())\n",
    "        lr.fit(X_transformed, Y_transformed)\n",
    "\n",
    "        #classify Test set\n",
    "        predicted = lr.predict(cv.transform(X_test))\n",
    "\n",
    "        #obtain metrics avg/total\n",
    "        #shape of metrics\n",
    "        #precision    recall  f1-score   amount_tested\n",
    "        metrics_by_exercise = classification_report(\n",
    "            le.transform(Y_test), predicted, target_names=le.classes_, digits=2).split()[-4:]\n",
    "        metrics_by_exercise = [str(exercise)] + metrics_by_exercise + [int(X_train.shape[0] + X_test.shape[0])]\n",
    "        linear_performance.loc[len(linear_performance)+1]= metrics_by_exercise\n",
    "        i += 1\n",
    "    except:\n",
    "        not_trained.append(exercise)\n",
    "        pass\n",
    "    \n",
    "print(\"Trained {} classifier of {}\".format(i, len(exercises)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3QqGeM81xopi"
   },
   "source": [
    "### 2.2 Metrics Results\n",
    "Below we show precision recall and f1-score of classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weighted_mean(linear_performance, ['precision', 'recall', 'f1-score'],'submission_amount')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "distribution_by_exercise.sort_values('submission_amount').to_csv('distribution_by_exercise.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_performance.to_csv('linear_regression_performance.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "Linear_Regression_Applied_to_Haskell_Exercises.ipynb",
   "provenance": [],
   "toc_visible": true,
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
